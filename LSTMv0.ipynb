{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 19:13:59.760098: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-02 19:13:59.760122: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "import io\n",
    "from keras import optimizers\n",
    "from keras.callbacks import  ModelCheckpoint\n",
    "import os\n",
    "\n",
    "#Principe de la détection d'anomalies par auto-encodeurs:\n",
    "\n",
    "#On essaye de reconstruire toutes les séquences du signal, de taille fixée, grâce à note auto-encodeur (voir l'image des 1 pour mieux comprendre)\n",
    "#Si le singal reconstruit par l'auto-encodeur est trop différent du signal de départ alors on dit qu'il y a anomalie\n",
    "\n",
    "#Autre méthode à creuser pour la détection d'anomalie:\n",
    "\n",
    "#transformer le signal en image, appliquer une transformée de fourier sur la série temporelle et utiliser un réseau Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_200506/61914224.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['gamma'] = scaler.transform(train[['gamma']])\n",
      "/tmp/ipykernel_200506/61914224.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['gamma'] = scaler.transform(test[['gamma']])\n"
     ]
    }
   ],
   "source": [
    "#On implémente sur les données complètes de gamma\n",
    "\n",
    "df = pd.read_csv('/home/lucien/Documents/projet-data-science-pollution-master/donnees_txt_par_mois/2015_gamma_concat.txt')\n",
    "\n",
    "#on remarque qu'il y a un très gros pics vers 1633\n",
    "# for val in df['gamma']:\n",
    "#     #si la valeur dépasse 1000 afficher le numéro de la ligne\n",
    "#     if val > 1000:\n",
    "#         print(df.index[df['gamma'] == val].tolist())\n",
    "\n",
    "#sns.lineplot(x=df['Date'], y=df['gamma'])\n",
    "#plt.show()\n",
    "\n",
    "#Si on avait pas déjà séparer train et test lors de la création des csv on le fait avec pandas, mais c'est plus long\n",
    "\n",
    "train, test = df.loc[df['Date'] <= 130000], df.loc[df['Date'] > 130000] #on coupe à 130 000 pour avoir un des gros pics que l'on remarque sur le graphique\n",
    "\n",
    "\n",
    "#Convert pandas dataframe to numpy array\n",
    "#dataset = dataframe.values\n",
    "#dataset = dataset.astype('float32') #COnvert values to float\n",
    "\n",
    "#LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
    "\n",
    "# normalize the dataset\n",
    "#scaler = MinMaxScaler() #Also try QuantileTransformer\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[['gamma']]) #Compute the mean and std to be used for later scaling.\n",
    "\n",
    "#on centre réduit nos valeurs\n",
    "\n",
    "train['gamma'] = scaler.transform(train[['gamma']])\n",
    "test['gamma'] = scaler.transform(test[['gamma']])\n",
    "\n",
    "\n",
    "#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "\n",
    "seq_size = 450  # Number of time steps to look back (on choisit 300 en s'appuyant sur notre travail avec la matrix profile)\n",
    "\n",
    "\n",
    "#Larger sequences (look further back) may improve forecasting.\n",
    "\n",
    "\n",
    "def to_sequences(x, y, seq_size=1):\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "\n",
    "    for i in range(len(x)-seq_size):\n",
    "        #print(i)\n",
    "        x_values.append(x.iloc[i:(i+seq_size)].values)\n",
    "        y_values.append(y.iloc[i+seq_size])\n",
    "        \n",
    "    return np.array(x_values), np.array(y_values)\n",
    "\n",
    "trainX, trainY = to_sequences(train[['gamma']], train['gamma'], seq_size) #on découpe en séquence de 300\n",
    "testX, testY = to_sequences(test[['gamma']], test['gamma'], seq_size)\n",
    "\n",
    "\n",
    "#ici pas auto-encodeur amis auto-régressif car on prédit pas un timestamp n à partir du n-ième mais on prédit n+1\n",
    "\n",
    "#REGARDER wave net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 19:14:19.650426: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-10-02 19:14:19.650449: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-C8Q1Q308): /proc/driver/nvidia/version does not exist\n",
      "2022-10-02 19:14:19.650788: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 256)               264192    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 450, 256)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 450, 256)          525312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 450, 256)          0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 450, 1)           257       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 789,761\n",
      "Trainable params: 789,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define Autoencoder model\n",
    "\n",
    "#first model\n",
    "#Input shape would be seq_size, 1 - 1 beacuse we have 1 feature. \n",
    "# seq_size = trainX.shape[1]\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(256, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "# model.add(LSTM(128, activation='relu', return_sequences=False))\n",
    "# model.add(RepeatVector(trainX.shape[1]))\n",
    "# model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "# model.add(LSTM(256, activation='relu', return_sequences=True))\n",
    "# model.add(TimeDistributed(Dense(trainX.shape[2])))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# model.summary()\n",
    "\n",
    "#second model\n",
    "#Try another model with one LSTM layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(trainX.shape[1], trainX.shape[2]))) #128 car on a 128 neurones dans la couche cachée (on va augmenter car 128 peut-être trop faible, c'est peut-être la raison de notre under-fitting)\n",
    "model.add(Dropout(rate=0.2)) #rate?\n",
    "\n",
    "model.add(RepeatVector(trainX.shape[1]))\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(TimeDistributed(Dense(trainX.shape[2])))\n",
    "\n",
    "#ici on devriat normalement mettre une couche avec une foction d'activation, mais là on met une fonction linéaire par défaut\n",
    "\n",
    "#optim = tf.keras.optimizers.SGD(learning_rate = 1)\n",
    "model.compile(optimizer='adam', loss='mse') #on rajoute un optimizer et une loss\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on sauvegarde les poids pour les charger plus tard\n",
    "\n",
    "checkpoint_path = \"/home/lucien/Documents/projet-data-science-pollution-master/weights/training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 19:15:04.807371: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 209871000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  17/3644 [..............................] - ETA: 1:27:18 - loss: 0.1753"
     ]
    }
   ],
   "source": [
    "#on lance le training\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainY, epochs=20, batch_size=32, validation_split=0.1, shuffle=False, callbacks=[cp_callback]) #nombre faible d'epoch pour l'instant, on veut juste tester\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On trace le résultat de notre train\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On évalue notre modèle sur l'ensemble test en utilisant les metrics de base\n",
    "\n",
    "model.evaluate(testX, testY) #on évalue le training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#Passons maintenant à la détection des anomalies sur notre propre signal, pour cela il est capital de définir un seuil de détection d'anomalie.\n",
    "#Pour commencer un fixe un treshold simple: 90% de la valeur de l'erreur maxime (au delà on considèrera qu'on aura une anomalie)\n",
    "\n",
    "trainPredict = model.predict(trainX)\n",
    "trainMAE = np.mean(np.abs(trainPredict - trainX), axis=1)\n",
    "plt.hist(trainMAE, bins=30) #donne  la moyenne des erreurs absolues calculée sur chaque channel de taille 300 qu'on a voulu prédire\n",
    "max_trainMAE = 0.5  #or Define 90% value of max as threshold. On l'a choisi à partir du graphique obtenu ci-dessus. (on prend la dernière valeur de l'abcisse de l'histogramme bleu qu'on vient de tracer)\n",
    "\n",
    "testPredict = model.predict(testX)\n",
    "testMAE = np.mean(np.abs(testPredict - testX), axis=1)\n",
    "plt.hist(testMAE, bins=30) #trace un histogramme rouge\n",
    "\n",
    "#Capture all details in a DataFrame for easy plotting\n",
    "anomaly_df = pd.DataFrame(test[seq_size:])\n",
    "anomaly_df['testMAE'] = testMAE\n",
    "anomaly_df['max_trainMAE'] = max_trainMAE\n",
    "anomaly_df['anomaly'] = anomaly_df['testMAE'] > anomaly_df['max_trainMAE'] #si cette inégalité est vérifié alors il y a anomalie car l'erreur moyenne est supérieure au treshold\n",
    "anomaly_df['gamma'] = test[seq_size:]['gamma']\n",
    "\n",
    "print(anomaly_df)\n",
    "\n",
    "anomalies = anomaly_df.loc[anomaly_df['anomaly'] == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On trace l'évolution de l'erreur moyenne en fonction du temps\n",
    "sns.lineplot(x=anomaly_df['Date'], y=anomaly_df['max_trainMAE'], color='g')#trace la ligne égale au treshold\n",
    "sns.lineplot(x=anomaly_df['Date'], y=anomaly_df['testMAE']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0n trace notre signal test en bleu et les anomalies sur notre échantillon test\n",
    "\n",
    "sns.lineplot(x=anomaly_df['Date'], y=anomaly_df['gamma'], color='b')#trace les valeurs de gamma pour l'échantillon test\n",
    "sns.scatterplot(x=anomalies['Date'], y=anomalies['gamma'], color='r') #trace les points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anomalies['Date'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
