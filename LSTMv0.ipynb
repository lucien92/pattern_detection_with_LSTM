{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "import io\n",
    "from keras import optimizers\n",
    "from keras.callbacks import  ModelCheckpoint\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On implémente sur les données complètes de gamma\n",
    "\n",
    "df = pd.read_csv('/home/lucien/Documents/projet-data-science-pollution-master/donnees_txt_par_mois/2015_gamma_concat.txt')\n",
    "\n",
    "#on remarque qu'il y a un très gros pics vers 1633\n",
    "# for val in df['gamma']:\n",
    "#     #si la valeur dépasse 1000 afficher le numéro de la ligne\n",
    "#     if val > 1000:\n",
    "#         print(df.index[df['gamma'] == val].tolist())\n",
    "\n",
    "#sns.lineplot(x=df['Date'], y=df['gamma'])\n",
    "#plt.show()\n",
    "\n",
    "#Si on avait pas déjà séparer train et test lors de la création des csv on le fait avec pandas, mais c'est plus long\n",
    "\n",
    "train, test = df.loc[df['Date'] <= 130000], df.loc[df['Date'] > 130000] #on coupe à 130 000 pour avoir un des gros pics que l'on remarque sur le graphique\n",
    "\n",
    "\n",
    "#Convert pandas dataframe to numpy array\n",
    "#dataset = dataframe.values\n",
    "#dataset = dataset.astype('float32') #COnvert values to float\n",
    "\n",
    "#LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
    "\n",
    "# normalize the dataset\n",
    "#scaler = MinMaxScaler() #Also try QuantileTransformer\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[['gamma']])\n",
    "\n",
    "train['gamma'] = scaler.transform(train[['gamma']])\n",
    "test['gamma'] = scaler.transform(test[['gamma']])\n",
    "\n",
    "\n",
    "#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "#In this example, the n_features is 2. We will make timesteps = 3. \n",
    "#With this, the resultant n_samples is 5 (as the input data has 9 rows).\n",
    "\n",
    "seq_size = 450  # Number of time steps to look back (on choisit 300 en s'appuyant sur notre travail avec la matrix profile)\n",
    "\n",
    "\n",
    "#Larger sequences (look further back) may improve forecasting.\n",
    "\n",
    "\n",
    "def to_sequences(x, y, seq_size=1):\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "\n",
    "    for i in range(len(x)-seq_size):\n",
    "        #print(i)\n",
    "        x_values.append(x.iloc[i:(i+seq_size)].values)\n",
    "        y_values.append(y.iloc[i+seq_size])\n",
    "        \n",
    "    return np.array(x_values), np.array(y_values)\n",
    "\n",
    "trainX, trainY = to_sequences(train[['gamma']], train['gamma'], seq_size) #on découpe en séquence de 300\n",
    "testX, testY = to_sequences(test[['gamma']], test['gamma'], seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Autoencoder model\n",
    "\n",
    "#first model\n",
    "#Input shape would be seq_size, 1 - 1 beacuse we have 1 feature. \n",
    "# seq_size = trainX.shape[1]\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(256, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "# model.add(LSTM(128, activation='relu', return_sequences=False))\n",
    "# model.add(RepeatVector(trainX.shape[1]))\n",
    "# model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "# model.add(LSTM(256, activation='relu', return_sequences=True))\n",
    "# model.add(TimeDistributed(Dense(trainX.shape[2])))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# model.summary()\n",
    "\n",
    "#second model\n",
    "#Try another model with one LSTM layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(trainX.shape[1], trainX.shape[2]))) #128 car on a 128 neurones dans la couche cachée (on va augmenter car 128 peut-être trop faible, c'est peut-être la raison de notre under-fitting)\n",
    "model.add(Dropout(rate=0.2)) #rate?\n",
    "\n",
    "model.add(RepeatVector(trainX.shape[1]))\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(TimeDistributed(Dense(trainX.shape[2])))\n",
    "\n",
    "#optim = tf.keras.optimizers.SGD(learning_rate = 1)\n",
    "model.compile(optimizer='adam', loss='mae') #on rajoute un optimizer et une loss\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on sauvegarde les poids pour les charger plus tard\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on lance le training\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainY, epochs=20, batch_size=32, validation_split=0.1, shuffle=False, callbacks=[cp_callback]) #nombre faible d'epoch pour l'instant, on veut juste tester\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On trace le résultat de notre train\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On évalue notre modèle sur l'ensemble test en utilisant les metrics de base\n",
    "\n",
    "model.evaluate(testX, testY) #on évalue le training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#Passons maintenant à la détection des anomalies sur notre propre signal, pour cela il est capital de définir un seuil de détection d'anomalie.\n",
    "#Pour commencer un fixe un treshold simple: 90% de la valeur de l'erreur maxime (au delà on considèrera qu'on aura une anomalie)\n",
    "\n",
    "trainPredict = model.predict(trainX)\n",
    "trainMAE = np.mean(np.abs(trainPredict - trainX), axis=1)\n",
    "plt.hist(trainMAE, bins=30) #donne  la moyenne des erreurs absolues calculée sur chaque channel de taille 300 qu'on a voulu prédire\n",
    "max_trainMAE = 0.5  #or Define 90% value of max as threshold. On l'a choisi à partir du graphique obtenu ci-dessus. (on prend la dernière valeur de l'abcisse de l'histogramme bleu qu'on vient de tracer)\n",
    "\n",
    "testPredict = model.predict(testX)\n",
    "testMAE = np.mean(np.abs(testPredict - testX), axis=1)\n",
    "plt.hist(testMAE, bins=30) #trace un histogramme rouge\n",
    "\n",
    "#Capture all details in a DataFrame for easy plotting\n",
    "anomaly_df = pd.DataFrame(test[seq_size:])\n",
    "anomaly_df['testMAE'] = testMAE\n",
    "anomaly_df['max_trainMAE'] = max_trainMAE\n",
    "anomaly_df['anomaly'] = anomaly_df['testMAE'] > anomaly_df['max_trainMAE'] #si cette inégalité est vérifié alors il y a anomalie car l'erreur moyenne est supérieure au treshold\n",
    "anomaly_df['gamma'] = test[seq_size:]['gamma']\n",
    "\n",
    "print(anomaly_df)\n",
    "\n",
    "anomalies = anomaly_df.loc[anomaly_df['anomaly'] == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On trace l'évolution de l'erreur moyenne en fonction du temps\n",
    "sns.lineplot(x=anomaly_df['Date'], y=anomaly_df['max_trainMAE'], color='g')#trace la ligne égale au treshold\n",
    "sns.lineplot(x=anomaly_df['Date'], y=anomaly_df['testMAE']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0n trace notre signal test en bleu et les anomalies sur notre échantillon test\n",
    "\n",
    "sns.lineplot(x=anomaly_df['Date'], y=anomaly_df['gamma'], color='b')#trace les valeurs de gamma pour l'échantillon test\n",
    "sns.scatterplot(x=anomalies['Date'], y=anomalies['gamma'], color='r') #trace les points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anomalies['Date'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
